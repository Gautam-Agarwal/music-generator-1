{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1SsKJ4WWK_2Oxj1vpxKLH0U9R56Sp-3J8",
      "authorship_tag": "ABX9TyPHCQ/3mHA1zn0N/dxyJFfk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gautam-Agarwal/music-generator-1/blob/main/music_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from music21 import converter, instrument, note, chord, stream\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import BatchNormalization as BatchNorm\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras import optimizers\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n"
      ],
      "metadata": {
        "id": "Cq0h-wDChCHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_songs(path):\n",
        "    # Loads MIDIs and converts them into a list of sequence of notes\n",
        "    songs = []\n",
        "    folder = Path(path)\n",
        "    for file in folder.rglob('*.mid'):\n",
        "        songs.append(file)\n",
        "\n",
        "    return songs"
      ],
      "metadata": {
        "id": "eLoKwwHKhFeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_songs(songs):\n",
        "    notes = []\n",
        "    for i,file in enumerate(songs):\n",
        "        print(f'{i+1}: {file}')\n",
        "        try:\n",
        "            midi = converter.parse(file)\n",
        "            notes_to_parse = None\n",
        "            parts = instrument.partitionByInstrument(midi)\n",
        "            if parts: # file has instrument parts\n",
        "                notes_to_parse = parts.parts[0].recurse()\n",
        "            else: # file has notes in a flat structure\n",
        "                notes_to_parse = midi.flat.notes\n",
        "            for element in notes_to_parse:\n",
        "                if isinstance(element, note.Note):\n",
        "                    notes.append(str(element.pitch))\n",
        "                elif isinstance(element, chord.Chord):\n",
        "                    notes.append('.'.join(str(n) for n in element.normalOrder))\n",
        "        except:\n",
        "            print(f'FAILED: {i+1}: {file}')\n",
        "\n",
        "\n",
        "    # Save notes to Drive for future usage\n",
        "    with open('notes', 'wb') as filepath:\n",
        "        pickle.dump(notes, filepath)\n",
        "\n"
      ],
      "metadata": {
        "id": "pHMNk1iwhRY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/myMidi'\n",
        "songs = load_songs(path)\n",
        "preprocess_songs(songs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BA93OR9hRRW",
        "outputId": "79b01ae9-f476-4e79-eb23-a9b8eaa67650"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: /content/drive/MyDrive/myMidi/TRAP EDM.mid\n",
            "2: /content/drive/MyDrive/myMidi/drum1.mid\n",
            "3: /content/drive/MyDrive/myMidi/Piano Piece Bridge.mid\n",
            "4: /content/drive/MyDrive/myMidi/Progression based.mid\n",
            "5: /content/drive/MyDrive/myMidi/C Major Wonder.logixc.mid\n",
            "6: /content/drive/MyDrive/myMidi/D major chord progression.mid\n",
            "7: /content/drive/MyDrive/myMidi/pop.mid\n",
            "8: /content/drive/MyDrive/myMidi/halftime.mid\n",
            "FAILED: 8: /content/drive/MyDrive/myMidi/halftime.mid\n",
            "9: /content/drive/MyDrive/myMidi/Final Comp.mid\n",
            "10: /content/drive/MyDrive/myMidi/Avenue.mid\n",
            "11: /content/drive/MyDrive/myMidi/Jazz.mid\n",
            "12: /content/drive/MyDrive/myMidi/Ballad.mid\n",
            "13: /content/drive/MyDrive/myMidi/horn chord beat.mid\n",
            "14: /content/drive/MyDrive/myMidi/beginning.mid\n",
            "15: /content/drive/MyDrive/myMidi/Rock.mid\n",
            "16: /content/drive/MyDrive/myMidi/RNB 2.mid\n",
            "17: /content/drive/MyDrive/myMidi/idk.mid\n",
            "18: /content/drive/MyDrive/myMidi/orchestral.mid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_notes():\n",
        "    with open('notes','rb') as filepath:\n",
        "        notes = pickle.load(filepath)\n",
        "\n",
        "    return notes"
      ],
      "metadata": {
        "id": "wE0-lIK_hRHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preocess_notes(notes):\n",
        "\n",
        "    dataSize = 150000\n",
        "    allNotes = notes\n",
        "    allNotes = allNotes[:dataSize]\n",
        "\n",
        "    # Look at 10 previous notes to make a prediction\n",
        "    #   We can tune this parameter if needed, based on the length of \n",
        "    #   chord progressions\n",
        "    seqLength = 10\n",
        "    print('Using sequence length of {}'.format(seqLength))\n",
        "\n",
        "    pitchSet = sorted(set(allNotes))\n",
        "    numPitches = len(pitchSet)\n",
        "    # here pitches are either notes or chords\n",
        "    #   they are sorted lexicographically, so a chord 'C4.E4' will come after a\n",
        "    #   note 'C4'\n",
        "    print('Identified {} pitches'.format(numPitches))\n",
        "\n",
        "    # Map each note/chord to a number normalized to (0,1)\n",
        "    pitchMapping = dict((note, number) for (number, note) in enumerate(pitchSet))\n",
        "\n",
        "    networkInput = []\n",
        "    networkOutput = []\n",
        "\n",
        "    print('Starting sequencing of {} notes'.format(len(allNotes)))\n",
        "    for i in range(0, len(allNotes)- seqLength):\n",
        "        sequenceIn = allNotes[i:i+seqLength]\n",
        "        predictionOut = allNotes[i+seqLength]\n",
        "\n",
        "        networkInput.append([pitchMapping[note] for note in sequenceIn])\n",
        "        networkOutput.append(pitchMapping[predictionOut])\n",
        "\n",
        "        if (i+1) % 400000 == 0:\n",
        "            print('Finished making {} sequences'.format(i+1))\n",
        "\n",
        "    networkInput = np.array(networkInput)\n",
        "    networkOutput = np.array(networkOutput)\n",
        "\n",
        "    numSeqs = len(networkInput)\n",
        "    # reshape input to match the LSTM layer format\n",
        "    networkInputShaped = np.reshape(networkInput, (numSeqs, seqLength, 1))\n",
        "    networkInputShaped = networkInputShaped / numPitches\n",
        "\n",
        "    networkOutputShaped = np_utils.to_categorical(networkOutput)\n",
        "\n",
        "    return networkInput, networkOutput, networkInputShaped, networkOutputShaped, numPitches\n"
      ],
      "metadata": {
        "id": "E7jnkjpUhRD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(networkInputShaped,networkOutputShaped,numPitches,num_epochs=30):\n",
        "    model = Sequential()\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(\n",
        "        512,\n",
        "        input_shape=(networkInputShaped.shape[1], networkInputShaped.shape[2]),\n",
        "        return_sequences=True\n",
        "    ))\n",
        "    model.add(Dense(256))\n",
        "    model.add(Dense(256))\n",
        "    model.add(LSTM(512, return_sequences=True))\n",
        "    model.add(Dense(256))\n",
        "    model.add(LSTM(512))\n",
        "    model.add(Dense(numPitches))\n",
        "    model.add(Dense(numPitches))\n",
        "    model.add(Activation('softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "    num_epochs = 30\n",
        "\n",
        "    filepath = \"/weights/weights-improvement-{epoch:02d}-{loss:.4f}-bigger_1.hdf5\"\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        filepath, monitor='loss', \n",
        "        verbose=1,        \n",
        "        save_best_only=True,        \n",
        "        mode='min'\n",
        "    )    \n",
        "    callbacks_list = [checkpoint]\n",
        "\n",
        "\n",
        "    history = model.fit(networkInputShaped, networkOutputShaped, epochs=num_epochs, batch_size=64, callbacks=callbacks_list)\n",
        "\n",
        "    return model, history\n"
      ],
      "metadata": {
        "id": "_tHHwyE4l17m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_notes(model, network_input, pitchnames, n_vocab):\n",
        "    \"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
        "    # pick a random sequence from the input as a starting point for the prediction\n",
        "    # Selects a random row from the network_input\n",
        "    start = np.random.randint(0, len(network_input)-1)\n",
        "    print(f'start: {start}')\n",
        "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
        "\n",
        "    # Random row from network_input\n",
        "    pattern = network_input[start]\n",
        "    prediction_output = []\n",
        "\n",
        "    # generate 500 notes\n",
        "    for note_index in range(500):\n",
        "        print(note_index)\n",
        "        # Reshapes pattern into a vector\n",
        "        prediction_input = np.reshape(pattern, (1, len(pattern), 1))\n",
        "        # Standarizes pattern\n",
        "        prediction_input = prediction_input / float(n_vocab)\n",
        "\n",
        "        # Predicts the next note\n",
        "        prediction = model.predict(prediction_input, verbose=0)\n",
        "\n",
        "        # Outputs a OneHot encoded vector, so this picks the columns\n",
        "        # with the highest probability\n",
        "        index = np.argmax(prediction)\n",
        "        # Maps the note to its respective index\n",
        "        result = int_to_note[index]\n",
        "        # Appends the note to the prediction_output\n",
        "        prediction_output.append(result)\n",
        "\n",
        "        # Adds the predicted note to the pattern\n",
        "        pattern = np.append(pattern,index)\n",
        "        # Slices the array so that it contains the predicted note\n",
        "        # eliminating the first from the array, so the model can\n",
        "        # have a sequence\n",
        "        pattern = pattern[1:len(pattern)]\n",
        "\n",
        "    return prediction_output\n"
      ],
      "metadata": {
        "id": "yQ1VSrmfl9Oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_midi(prediction_output):\n",
        "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
        "        from the notes \"\"\"\n",
        "    offset = 0\n",
        "    output_notes = []\n",
        "\n",
        "    # create note and chord objects based on the values generated by the model\n",
        "    for pattern in prediction_output:\n",
        "        # pattern is a chord\n",
        "        if ('.' in pattern) or pattern.isdigit():\n",
        "            notes_in_chord = pattern.split('.')\n",
        "            notes = []\n",
        "            for current_note in notes_in_chord:\n",
        "                new_note = note.Note(int(current_note))\n",
        "                new_note.storedInstrument = instrument.Piano()\n",
        "                notes.append(new_note)\n",
        "            new_chord = chord.Chord(notes)\n",
        "            new_chord.offset = offset\n",
        "            output_notes.append(new_chord)\n",
        "        # pattern is a note\n",
        "        else:\n",
        "            new_note = note.Note(pattern)\n",
        "            new_note.offset = offset\n",
        "            new_note.storedInstrument = instrument.Piano()\n",
        "            output_notes.append(new_note)\n",
        "\n",
        "        # increase offset each iteration so that notes do not stack\n",
        "        offset += 0.5\n",
        "\n",
        "    midi_stream = stream.Stream(output_notes)\n",
        "\n",
        "    midi_stream.write('midi', fp='output.mid')\n"
      ],
      "metadata": {
        "id": "_lXNop6cmBKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notes = load_notes()\n",
        "\n",
        "n_vocab = len(set(notes))\n",
        "print(n_vocab)\n",
        "pitchnames = sorted(set(item for item in notes))\n",
        "\n",
        "networkInput, networkOutput, networkInputShaped, networkOutputShaped, numPitches = preocess_notes(notes)\n",
        "model, history = create_model(networkInputShaped,networkOutputShaped,numPitches,num_epochs=30)\n",
        "prediction_output = generate_notes(model, networkInputShaped, pitchnames, n_vocab)\n",
        "\n",
        "create_midi(prediction_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zC-7k2imIuL",
        "outputId": "ace780b1-f000-4ce4-dcee-3222cc041f98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "169\n",
            "Using sequence length of 10\n",
            "Identified 169 pitches\n",
            "Starting sequencing of 30786 notes\n",
            "Epoch 1/30\n",
            "481/481 [==============================] - ETA: 0s - loss: 4.2354 - accuracy: 0.0820\n",
            "Epoch 1: loss improved from inf to 4.23540, saving model to /weights/weights-improvement-01-4.2354-bigger_1.hdf5\n",
            "481/481 [==============================] - 369s 748ms/step - loss: 4.2354 - accuracy: 0.0820\n",
            "Epoch 2/30\n",
            "481/481 [==============================] - ETA: 0s - loss: 4.0140 - accuracy: 0.0987\n",
            "Epoch 2: loss improved from 4.23540 to 4.01397, saving model to /weights/weights-improvement-02-4.0140-bigger_1.hdf5\n",
            "481/481 [==============================] - 352s 732ms/step - loss: 4.0140 - accuracy: 0.0987\n",
            "Epoch 3/30\n",
            "481/481 [==============================] - ETA: 0s - loss: 3.6805 - accuracy: 0.1565\n",
            "Epoch 3: loss improved from 4.01397 to 3.68049, saving model to /weights/weights-improvement-03-3.6805-bigger_1.hdf5\n",
            "481/481 [==============================] - 353s 734ms/step - loss: 3.6805 - accuracy: 0.1565\n",
            "Epoch 4/30\n",
            "481/481 [==============================] - ETA: 0s - loss: 3.4426 - accuracy: 0.1990\n",
            "Epoch 4: loss improved from 3.68049 to 3.44258, saving model to /weights/weights-improvement-04-3.4426-bigger_1.hdf5\n",
            "481/481 [==============================] - 356s 740ms/step - loss: 3.4426 - accuracy: 0.1990\n",
            "Epoch 5/30\n",
            "481/481 [==============================] - ETA: 0s - loss: 3.2408 - accuracy: 0.2383\n",
            "Epoch 5: loss improved from 3.44258 to 3.24079, saving model to /weights/weights-improvement-05-3.2408-bigger_1.hdf5\n",
            "481/481 [==============================] - 355s 738ms/step - loss: 3.2408 - accuracy: 0.2383\n",
            "Epoch 6/30\n",
            "481/481 [==============================] - ETA: 0s - loss: 3.0767 - accuracy: 0.2669\n",
            "Epoch 6: loss improved from 3.24079 to 3.07668, saving model to /weights/weights-improvement-06-3.0767-bigger_1.hdf5\n",
            "481/481 [==============================] - 351s 729ms/step - loss: 3.0767 - accuracy: 0.2669\n",
            "Epoch 7/30\n",
            "481/481 [==============================] - ETA: 0s - loss: 2.9340 - accuracy: 0.2971\n",
            "Epoch 7: loss improved from 3.07668 to 2.93401, saving model to /weights/weights-improvement-07-2.9340-bigger_1.hdf5\n",
            "481/481 [==============================] - 352s 732ms/step - loss: 2.9340 - accuracy: 0.2971\n",
            "Epoch 8/30\n",
            "481/481 [==============================] - ETA: 0s - loss: 2.8110 - accuracy: 0.3226\n",
            "Epoch 8: loss improved from 2.93401 to 2.81103, saving model to /weights/weights-improvement-08-2.8110-bigger_1.hdf5\n",
            "481/481 [==============================] - 355s 738ms/step - loss: 2.8110 - accuracy: 0.3226\n",
            "Epoch 9/30\n",
            "481/481 [==============================] - ETA: 0s - loss: 2.6856 - accuracy: 0.3448\n",
            "Epoch 9: loss improved from 2.81103 to 2.68557, saving model to /weights/weights-improvement-09-2.6856-bigger_1.hdf5\n",
            "481/481 [==============================] - 342s 711ms/step - loss: 2.6856 - accuracy: 0.3448\n",
            "Epoch 10/30\n",
            "481/481 [==============================] - ETA: 0s - loss: 2.5840 - accuracy: 0.3657\n",
            "Epoch 10: loss improved from 2.68557 to 2.58399, saving model to /weights/weights-improvement-10-2.5840-bigger_1.hdf5\n",
            "481/481 [==============================] - 350s 727ms/step - loss: 2.5840 - accuracy: 0.3657\n",
            "Epoch 11/30\n",
            "481/481 [==============================] - ETA: 0s - loss: 2.4966 - accuracy: 0.3847\n",
            "Epoch 11: loss improved from 2.58399 to 2.49659, saving model to /weights/weights-improvement-11-2.4966-bigger_1.hdf5\n",
            "481/481 [==============================] - 353s 733ms/step - loss: 2.4966 - accuracy: 0.3847\n",
            "Epoch 12/30\n",
            "481/481 [==============================] - ETA: 0s - loss: 2.4120 - accuracy: 0.3981\n",
            "Epoch 12: loss improved from 2.49659 to 2.41205, saving model to /weights/weights-improvement-12-2.4120-bigger_1.hdf5\n",
            "481/481 [==============================] - 353s 735ms/step - loss: 2.4120 - accuracy: 0.3981\n",
            "Epoch 13/30\n",
            "481/481 [==============================] - ETA: 0s - loss: 2.3280 - accuracy: 0.4191\n",
            "Epoch 13: loss improved from 2.41205 to 2.32797, saving model to /weights/weights-improvement-13-2.3280-bigger_1.hdf5\n",
            "481/481 [==============================] - 352s 731ms/step - loss: 2.3280 - accuracy: 0.4191\n",
            "Epoch 14/30\n",
            "481/481 [==============================] - ETA: 0s - loss: 2.2474 - accuracy: 0.4338\n",
            "Epoch 14: loss improved from 2.32797 to 2.24744, saving model to /weights/weights-improvement-14-2.2474-bigger_1.hdf5\n",
            "481/481 [==============================] - 344s 716ms/step - loss: 2.2474 - accuracy: 0.4338\n",
            "Epoch 15/30\n",
            "481/481 [==============================] - ETA: 0s - loss: 2.1711 - accuracy: 0.4473\n",
            "Epoch 15: loss improved from 2.24744 to 2.17107, saving model to /weights/weights-improvement-15-2.1711-bigger_1.hdf5\n",
            "481/481 [==============================] - 354s 735ms/step - loss: 2.1711 - accuracy: 0.4473\n",
            "Epoch 16/30\n",
            "481/481 [==============================] - ETA: 0s - loss: 2.1196 - accuracy: 0.4564\n",
            "Epoch 16: loss improved from 2.17107 to 2.11958, saving model to /weights/weights-improvement-16-2.1196-bigger_1.hdf5\n",
            "481/481 [==============================] - 349s 725ms/step - loss: 2.1196 - accuracy: 0.4564\n",
            "Epoch 17/30\n",
            "481/481 [==============================] - ETA: 0s - loss: 2.0648 - accuracy: 0.4654\n",
            "Epoch 17: loss improved from 2.11958 to 2.06479, saving model to /weights/weights-improvement-17-2.0648-bigger_1.hdf5\n",
            "481/481 [==============================] - 350s 728ms/step - loss: 2.0648 - accuracy: 0.4654\n",
            "Epoch 18/30\n",
            "481/481 [==============================] - ETA: 0s - loss: 2.0010 - accuracy: 0.4845\n",
            "Epoch 18: loss improved from 2.06479 to 2.00095, saving model to /weights/weights-improvement-18-2.0010-bigger_1.hdf5\n",
            "481/481 [==============================] - 351s 730ms/step - loss: 2.0010 - accuracy: 0.4845\n",
            "Epoch 19/30\n",
            "481/481 [==============================] - ETA: 0s - loss: 1.9513 - accuracy: 0.4961\n",
            "Epoch 19: loss improved from 2.00095 to 1.95128, saving model to /weights/weights-improvement-19-1.9513-bigger_1.hdf5\n",
            "481/481 [==============================] - 352s 732ms/step - loss: 1.9513 - accuracy: 0.4961\n",
            "Epoch 20/30\n",
            "481/481 [==============================] - ETA: 0s - loss: 1.8948 - accuracy: 0.5075\n",
            "Epoch 20: loss improved from 1.95128 to 1.89485, saving model to /weights/weights-improvement-20-1.8948-bigger_1.hdf5\n",
            "481/481 [==============================] - 357s 742ms/step - loss: 1.8948 - accuracy: 0.5075\n",
            "Epoch 21/30\n",
            "481/481 [==============================] - ETA: 0s - loss: 1.8622 - accuracy: 0.5153\n",
            "Epoch 21: loss improved from 1.89485 to 1.86216, saving model to /weights/weights-improvement-21-1.8622-bigger_1.hdf5\n",
            "481/481 [==============================] - 361s 751ms/step - loss: 1.8622 - accuracy: 0.5153\n",
            "Epoch 22/30\n",
            "481/481 [==============================] - ETA: 0s - loss: 1.8162 - accuracy: 0.5225\n",
            "Epoch 22: loss improved from 1.86216 to 1.81616, saving model to /weights/weights-improvement-22-1.8162-bigger_1.hdf5\n",
            "481/481 [==============================] - 368s 764ms/step - loss: 1.8162 - accuracy: 0.5225\n",
            "Epoch 23/30\n",
            "481/481 [==============================] - ETA: 0s - loss: 1.7734 - accuracy: 0.5313\n",
            "Epoch 23: loss improved from 1.81616 to 1.77342, saving model to /weights/weights-improvement-23-1.7734-bigger_1.hdf5\n",
            "481/481 [==============================] - 361s 751ms/step - loss: 1.7734 - accuracy: 0.5313\n",
            "Epoch 24/30\n",
            "481/481 [==============================] - ETA: 0s - loss: 1.7369 - accuracy: 0.5374\n",
            "Epoch 24: loss improved from 1.77342 to 1.73690, saving model to /weights/weights-improvement-24-1.7369-bigger_1.hdf5\n",
            "481/481 [==============================] - 356s 740ms/step - loss: 1.7369 - accuracy: 0.5374\n",
            "Epoch 25/30\n",
            "481/481 [==============================] - ETA: 0s - loss: 1.7024 - accuracy: 0.5483\n",
            "Epoch 25: loss improved from 1.73690 to 1.70237, saving model to /weights/weights-improvement-25-1.7024-bigger_1.hdf5\n",
            "481/481 [==============================] - 354s 736ms/step - loss: 1.7024 - accuracy: 0.5483\n",
            "Epoch 26/30\n",
            "481/481 [==============================] - ETA: 0s - loss: 1.6669 - accuracy: 0.5589\n",
            "Epoch 26: loss improved from 1.70237 to 1.66691, saving model to /weights/weights-improvement-26-1.6669-bigger_1.hdf5\n",
            "481/481 [==============================] - 357s 743ms/step - loss: 1.6669 - accuracy: 0.5589\n",
            "Epoch 27/30\n",
            "481/481 [==============================] - ETA: 0s - loss: 1.6280 - accuracy: 0.5656\n",
            "Epoch 27: loss improved from 1.66691 to 1.62805, saving model to /weights/weights-improvement-27-1.6280-bigger_1.hdf5\n",
            "481/481 [==============================] - 355s 737ms/step - loss: 1.6280 - accuracy: 0.5656\n",
            "Epoch 28/30\n",
            "481/481 [==============================] - ETA: 0s - loss: 1.6112 - accuracy: 0.5695\n",
            "Epoch 28: loss improved from 1.62805 to 1.61120, saving model to /weights/weights-improvement-28-1.6112-bigger_1.hdf5\n",
            "481/481 [==============================] - 361s 751ms/step - loss: 1.6112 - accuracy: 0.5695\n",
            "Epoch 29/30\n",
            "481/481 [==============================] - ETA: 0s - loss: 1.5682 - accuracy: 0.5790\n",
            "Epoch 29: loss improved from 1.61120 to 1.56825, saving model to /weights/weights-improvement-29-1.5682-bigger_1.hdf5\n",
            "481/481 [==============================] - 351s 729ms/step - loss: 1.5682 - accuracy: 0.5790\n",
            "Epoch 30/30\n",
            "481/481 [==============================] - ETA: 0s - loss: 1.5570 - accuracy: 0.5843\n",
            "Epoch 30: loss improved from 1.56825 to 1.55697, saving model to /weights/weights-improvement-30-1.5570-bigger_1.hdf5\n",
            "481/481 [==============================] - 343s 712ms/step - loss: 1.5570 - accuracy: 0.5843\n",
            "start: 13754\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "490\n",
            "491\n",
            "492\n",
            "493\n",
            "494\n",
            "495\n",
            "496\n",
            "497\n",
            "498\n",
            "499\n"
          ]
        }
      ]
    }
  ]
}